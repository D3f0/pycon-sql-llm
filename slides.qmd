---
title: "💬 ❓ 🤖 Asking questions to your database with LLMs "
# subtitle: "Techniques for effective SQL generation"
author: Nahuel Defossé <br>[nahuel.defosse@ibm.com](mailto:nahuel.defosse@ibm.com)<br>IBM Research Kenya Lab
date: September, 2025
embed-resources: false
footer: "*Asking question to {{< bi database >}} with LLMs* - 🐍 PyCon 🇰🇪 2025"
# https://quarto.org/docs/computations/caching.html
execute: 
  cache: true
  keep-ipynb: true
format:
  revealjs:
    toc: false
    toc-depth: 1
    slide-number: true
    transition: slide
    code-copy: true
    highlight-style: github
    code-overflow: wrap
    theme: [solarized, custom.scss]
    # For the reveal-logo extension
    # header-logo-left: ./img/Python-logo-notext.svg
    # header-logo-left-url: https://quarto.org
    # header-logo-left-height: 2em
    header-logo-right: ./img/Python-logo-notext.svg
    header-logo-right-url: https://pycon-kenya-2025.sessionize.com/
    header-logo-right-height: 50px


filters:
 - reveal-logo
---

## About myself {.smaller}

:::: {.columns}
::: {.column width="40%"}
![](./img/memes/myself.gif){height="12em"}
:::

::: {.column width="60%"}

::: {.incremental}
- 🐍 Pythonista with 18 years of experience.
  - Co-organized SciPy Latin America
- 🚜 Worked as CTO in  Hello Tractor 
- 🧪 Software Engineer at IBM Research 
- 🛰️ Worked in Foundational Models for Geospatial applications
- 💬 Currently working on [Flowpilot {{< bi link-45deg >}}](https://research.ibm.com/projects/flowpilot), providing core features to 
  different products and divisions.
::: 

:::
::::


---

## Follow along (or at 🏡) {.center}

{{< qrcode https://jmbuhr.de >}}


---
# Intro

In this talk we're gonna show how to use Python to:

  - Connect to a database and execute the queries
  - Convert natural language questions into SQL
  - Create a workflow
  - Fix common errors
  - Lessons learned
  
TODO: Put some drawing & rephrase

---

# Connect to a database and execute the queries


---

## Public datasets used in text to SQL

:::: {.columns}
::: {.column}
::: {.fragment .fade-right}

:::

::: {.fragment .fade-right}
- 🕷️ Spider
:::
::: {.fragment .fade-right}
- 🕷️ 🕷️ Spider 2
:::
::: {.fragment .fade-right}
- 🐦 BIRD
:::
::: {.fragment .fade-right}
- 🏹 Archer 
:::
:::
::: {.column}
::: {.fragment .fade-left .small_text}
These datasets define:

- ❓ Natural language questions
- 🫰 Expected SQL
- 🏗️ Database schema & content
- 🔎 Evaluation metrics 
- 🥇 Leaderboard
:::
:::
::::


::: notes
:::

---

## BIRD 

![](./img/ibm-granite-leaderboard.png){height="400px" fig-align="center"}

[{{<bi article>}} article](https://research.ibm.com/blog/granite-LLM-text-to-SQL)


::: notes
We're select BIRD dataset since we have some experience with it, we managed
to get to the top of the leatherboard in 2024/6
:::

---

### BIRD

- https://github.com/bird-bench/mini_dev

```{python}
#| echo: true
#| output-location: slide
from datasets import load_dataset, DownloadConfig 

# Load the dataset
dataset = load_dataset(
  "birdsql/bird_mini_dev", download_config=DownloadConfig(disable_tqdm=True)
)

# Contents
print("Database types: ", *dataset.keys())
sqlite_df = dataset["mini_dev_sqlite"].to_pandas()
display(sqlite_df.head(2))
```

---

### Downloading BIRD databases

<!-- - We will be using [`uvx`](https://docs.astral.sh/uv/concepts/tools/#tool-versions)[^uvx] with the `gdown` package as follows: -->

```shell
uvx gdown https://drive.google.com/file/d/13VLWIwpw5E3d5DUkMvzw7hvHE67a4XkG/
```
![](./img/download_minidev.png)
<!-- Or just download from the [ {{< bi google >}} {{< bi hdd >}} link](https://drive.google.com/file/d/13VLWIwpw5E3d5DUkMvzw7hvHE67a4XkG/view?usp=sharing)  -->

Extracting the archive (3.3GiB)

```shell
unzip minidev_703.zip
```
<!-- [^uvx]: It comes as part of `uv`, it's a shorthand for `uv tool run` -->

::: footer
:::
---

### Picking the example database `california_schools`

In `minidev/MINIDEV/dev_databases/california_schools/` we find the {{< bi database >}} {{< bi file >}}

![](./img/california_school.png)


---

### Creating the `Engine`

```{python}
#| echo: true
#| code-overflow: wrap

from sqlalchemy import create_engine
path = './minidev/MINIDEV/dev_databases/california_schools/california_schools.sqlite'
engine = create_engine(f'sqlite:///{path}')
engine
```


---

### Grabbing a simple question and its  {{< bi database >}} 

```{python}
#| echo: true
california_schools_df = sqlite_df[sqlite_df.db_id == 'california_schools']
simple_queries_df = california_schools_df[sqlite_df.difficulty == 'simple']
simple_queries_df.head(2).set_index("db_id")

```

---

```{python}
#| echo: true
import pandas as pd
pd.set_option('display.max_colwidth', 0)

question_sql_df = simple_queries_df[["question","SQL"]].reset_index()
# question_sql_df[0:1].T
```

::: {.fragment .fade-left}
```{python}
#| echo: true

question = question_sql_df.loc[0, "question"]
query = question_sql_df.loc[0, "SQL"]
print(f"{question!r}")
print(f"{query!r}")
```
:::

---

### Execute the queries

```{python}
#| echo: true

from sqlalchemy import text
with engine.connect() as conn:
    result = conn.execute(text(query))
    res_df = pd.DataFrame(result.fetchall()) # 🐼 ✨
    display(question)
    display(query)
    display(res_df)
```

---

### Database schema with 🦜 ⛓️

LangChain (🦜 ⛓️) community 🐍 📦 provides a simple class that can retrieve some schema information [^qa_lc]

```{python}
#| echo: true
!uv pip install langchain-community
```

```{python}
#| echo: true

from langchain_community.utilities import SQLDatabase
db = SQLDatabase(engine=engine)

display(db.get_usable_table_names())
```

::: {.fragment .slide-left}
As we can see, the table names may not be immediately understandable 🤔
:::
[^qa_lc]: [SQL Question Answering](https://python.langchain.com/docs/tutorials/sql_qa/#system-prompt){target="_blank"}

::: notes

:::

---

# Convert natural language questions into SQL

::: {.fragment .fade-in-then-semi-out}
LLMs are quite capable of writing functional SQL queries.
:::


::: {.fragment .fade-in-then-semi-out}
For those of us who have been writing code for some time and 
fell in love with ORMs when they were the *hot* new thing, LLMs
can take us to the next level!
:::

::: {.fragment .fade-in-then-semi-out}
But they don't know the 🏗️ structure of our database, and may hallucinate 
about it, or create some flat out invalid SQL, so we will need to handle
some of this scenarios.
:::

::: {.fragment .fade-in-then-semi-out}
Some research papers from our team from our team:

[{{< bi filetype-pdf >}} Weakly Supervised Detection of Hallucinations in LLMs](https://arxiv.org/pdf/2312.02798)

[{{< bi filetype-pdf >}} Localizing Persona Representations In LLMs](https://arxiv.org/pdf/2505.24539)
:::

::: notes
In Flowpilot we created a framework inspired on LangGraph for this.
:::

---

### Messages ✍️
<!-- ::: {.fragment .fade-in-then-semi-out}
There are some reasonably good LLMs under the coder and instruct in Hugging Face.
Some of these can be run locally with some inference server like `Ollama`, 
`llama.cpp` or `LMStudio`, and also use pubic 
ones.
:::

::: {.fragment .fade-in-then-semi-out}
Running LLMs locally or on-prem scenarios is quite important when
we need to preserve the confidentiality of the data. 
:::

::: {.fragment .fade-in-then-semi-out}
We'll take a look at a library/proxy that can help us move between LLMs
called `litellm` (can also add observability, cost tracking, etc.).
::: -->




---

### Contacting LLMs with `littellm` {.smaller}

::: {style="font-size: 90%;"}
::: {.fragment .fade-in-then-semi-out}

Installing *litellm* is a simple as

`uv add littellm` 

There are some `extras` [^extras]

:::

[^extras]: 
  
  `caching` , `extra-proxy` , `mlflow` , `proxy` , `semantic-router`, `utils`

:::: {.columns}
::: {.fragment .fade-in}
::: {.column width="70%"}
```python
import litellm

response = litellm.completion(
  model="ollama/granite-code:20b",
  messages=[
    {
      "role": "user",
      "content": "How are you?"
    }
  ],
  # stream=False
)
print(response) # or response
```
:::


::: {.column width="5%" }
:::

::: {.column width="25%" }
[{{< bi info-circle >}} `model` list](https://docs.litellm.ai/docs/providers)

:::
:::
:::

:::

---

### litellm response object

::: {.fragment .fade-in-then-semi-out}
```python
ModelResponse(
  id='chatcmpl-c34aae0e-a299-42a0-922e-aabd96e3a91e', 
  created=1758269174, 
  model='ollama/granite-code:20b', 
  object='chat.completion', 
  system_fingerprint=None, 
  choices=[Choices(
    finish_reason='stop', 
    index=0, 
    message=Message(
      content='### Bot:\nHey there! How can I assist you today?\n\n', 
      role='assistant', 
      tool_calls=None, 
      function_call=None, 
      provider_specific_fields=None
      )
    )], 
    usage=Usage(
      completion_tokens=17, 
      prompt_tokens=20, 
      total_tokens=37, 
      completion_tokens_details=None, 
      prompt_tokens_details=None
      )
    )
```
:::




---

### Getting some completions (cont.) {.center}
```python
#| echo: true
#| output: true
#| 
from litellm import completion

messages = [
  {"role": "user", 
  "content": "Create a small customer table in SQL"}]
response = completion(
  model="ollama/hf.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF:F16", 
  messages=messages, stream=False
)
print(response['choices'][0]['message']['content'])
```


---
# Create a workflow

---

# Chaining generation and execution

::: {.fragment .fade-in-then-semi-out}
When our code starts to become larger than a script can handle 🏋🏾‍♀️, `LangGraph` is a great
tool to provide some reusable organization. We can add it with `uv add langgraph`.
:::

::: {.fragment .fade-in-then-semi-out}
This will help us to separate some concerns like  📐 *validation* , 🏃🏽 *execution* and later,  ☢️ *enrichment*.
:::

::: {.fragment .fade-in-then-semi-out .center}
Let's see how we can create something like this:
```{mermaid}
%%| fig-align: center
flowchart LR
    S[Start]
    CallLLM[Call litellm's completion]
    S -->|State: Dict| CallLLM
    CallLLM --> End
    
```
:::

---

## State

LangGraph uses state that is propagated through nodes.

This state can be defined with a TypedDict or a Pydantic BaseModel.

```{python}
#| echo: true
from pydantic import BaseModel, Field
from typing import Optional

class State(BaseModel):
    question: str = ""
    schema: str = ""
    sql: Optional[str] = Field(default=None, description="")

```
---

# Fix common errors
## Adding a node for safety

::: notes
Usually done through LLMs in agentic workflows, we are going to take a
more deterministic approach.
:::

## Let's go for Bobby tables {.center}

![](./img/memes/exploits_of_a_mom.png)

::: footer
[`xkcd 327`](https://www.explainxkcd.com/wiki/index.php/327:_Exploits_of_a_Mom)
::: 

---

# Adding more information in the prompt

## Acronyms



---
# Lessons learned

---

## Reducing the schema for simpler queries


## Metadata about the columns

## Dynamic context
