---
title: 'üí¨ {{< bi database >}} ü§ñ Asking questions to your database with LLMs '
author: Nahuel Defoss√© {{< bi envelope >}} [nahuel.defosse@ibm.com](mailto:nahuel.defosse@ibm.com)<br>IBM Research Africa
#date: September, 2025
embed-resources: false
footer: '*Asking question to {{< bi database >}} with LLMs* - üêç PyCon üá∞üá™ 2025'
execute:
  cache: true
  keep-ipynb: true
format:
  revealjs:
    toc: false
    toc-depth: 1
    slide-number: true
    transition: slide
    code-copy: true
    highlight-style: a11y
    code-overflow: wrap
    theme:
      - solarized
      - custom.scss
    # header-logo-right: ./img/pynbo.png
    # header-logo-right-url: https://pycon-kenya-2025.sessionize.com/
    # header-logo-right-height: 2em
    preview-links: auto
filters:
  - reveal-logo
  - include-code-files
jupyter: python3

---
```{python}
import os
os.environ["JUPYTER_COLUMNS"] = "80"
# %load_ext rich
```
## About myself {.smaller}

:::: {.columns}
::: {.column width="40%"}
:::: {.content-visible when-format="html"}
![](./img/memes/myself.gif){height="12em"}
::::
:::

::: {.column width="60%"}

::: {.incremental}
- üêç Pythonista with 18 years of experience
  - Co-organized SciPy Latin America
  - PyCon Argentina and EuroPython speaker
- üöú Worked as CTO in  Hello Tractor 
- üß™ Software Engineer at IBM Research 
- üõ∞Ô∏è Worked in Foundational Models for Geospatial applications
- üí¨ Currently working on [Flowpilot {{< bi link-45deg >}}](https://research.ibm.com/projects/flowpilot), providing core features to 
  different products and divisions
::: 

:::
::::

---

::: {.content-visible when-format="html"}

## Tomorrow {.center}

::: {.columns}
::: {.column width="30%"}
![](./img/promo/wanjiru.jpg)
:::
::: {.column width="30%"}
![](./img/promo/beldine.jpg)
:::
::: {.column width="30%"}
![](./img/promo/reggie.png)
:::
::: 

Learn about [Multimodal Geospatial Foundation Models with Terratorch](https://pycon-kenya-2025.sessionize.com/session/954933){target="_blank"}
 by Wanjiru, Beldine and Reggie

:::

---

::: {.content-visible when-format="html"}

## Follow along (or at üè°) {.center}

{{< qrcode https://ibm.com >}}

:::

---

# Agenda

In this talk we're gonna show how to use Python to:

  - {{< bi database >}} Connect to a database and execute the queries
  - {{< bi filetype-sql >}} Convert natural language questions into SQL
  - {{< bi diagram-3 >}} Create a workflow
  - {{< bi database-gear >}} Managing configuration
  - {{< bi bookmark-check >}} Lessons learned

---

## High level diagram

![](./img/LLMsSQL.svg)



---

# Connect to a database and execute the queries

---

## Public datasets used in text to SQL

:::: {.columns}

::: {.column}
::: {.fragment .fade-left .small_text}
These datasets define:

- ‚ùì Natural language questions
- ü´∞ Expected SQL
- üèóÔ∏è Database schema & content
- üîé Evaluation metrics 
- ü•á Leaderboard
:::
:::

::: {.column}

::: {.fragment .fade-right}
- [üï∑Ô∏è Spider](https://yale-lily.github.io/spider){target="_bank"}
:::
::: {.fragment .fade-right}
- [üï∑Ô∏è üï∑Ô∏è Spider 2](https://spider2-sql.github.io/){target="_bank"}
:::
::: {.fragment .fade-right}
- üê¶ [BIRD](https://bird-bench.github.io/){target="_bank"}
:::
::: {.fragment .fade-right}
- [üèπ Archer](https://sig4kg.github.io/archer-bench/){target="_bank"}
:::
:::

::::


::: notes
:::

---

## BIRD 

![[IBM Research in the leaderboard last year](https://research.ibm.com/blog/granite-LLM-text-to-SQL)](./img/ibm-granite-leaderboard.png){height="400px" fig-align="center"}


::: notes
We're select BIRD dataset since we have some experience with it, we managed
to get to the top of the leatherboard in 2024/6
:::

---

### BIRD mini-dev

- It consist of 500 queries classified as **simple**, **moderate** and **challenging** [{{< bi box-arrow-up-right >}}](https://github.com/bird-bench/mini_dev)

::: {.fragment .fade-left}

<!-- Setup cell without output with imports -->
```{python}
from rich.console import Console
_c = Console(width=45)
# Ensure print are contained within the range
print = _c.print


from IPython.display import display, Image

```

```{python}
#| echo: true
#| output-location: slide
from datasets import load_dataset, DownloadConfig 

# Load the dataset
dataset = load_dataset(
  "birdsql/bird_mini_dev", 
  download_config=DownloadConfig(disable_tqdm=True)
)

# Only SQLite (there's Postgres and MySQL in the same dataset)
sqlite_df = (dataset["mini_dev_sqlite"]
  .to_pandas()
  .pipe(lambda df: df.drop(columns=['question_id'])))
display(sqlite_df.head(5))
```

:::

---

### Downloading BIRD databases

<!-- - We will be using [`uvx`](https://docs.astral.sh/uv/concepts/tools/#tool-versions)[^uvx] with the `gdown` package as follows: -->

```shell
uvx gdown https://drive.google.com/file/d/13VLWIwpw5E3d5DUkMvzw7hvHE67a4XkG/
```
![](./img/download_minidev.png)
<!-- Or just download from the [ {{< bi google >}} {{< bi hdd >}} link](https://drive.google.com/file/d/13VLWIwpw5E3d5DUkMvzw7hvHE67a4XkG/view?usp=sharing)  -->

Extracting the archive (3.3GiB)

```shell
unzip minidev_703.zip
```
<!-- [^uvx]: It comes as part of `uv`, it's a shorthand for `uv tool run` -->

::: footer
:::
---

### Picking the example database `california_schools`

In `minidev/MINIDEV/dev_databases/california_schools/` we find the {{< bi database >}} {{< bi file >}}

![](./img/california_school.png)

```{python}
#| echo: true
#| code-overflow: wrap
from pathlib import Path
base = Path('./minidev/MINIDEV/dev_databases/california_schools/')
```

---

### Creating the `Engine`

Creating an `Engine` instance connected to a SQLite database
```{python}
#| echo: true
#| code-overflow: wrap

from sqlalchemy import create_engine
db_path = base / 'california_schools.sqlite'
db_url = f"sqlite:///{db_path}"
engine = create_engine(db_url)
engine
```

---

## ER of `california_schools` {{< bi database >}}

```{python}
#from eralchemy import render_er
#render_er(db_url, './img/output/erd_from_sqlite.svg')

```
![](./img/er-california-schools.png){.lightbox}

::: footer
:::

---

### Getting one simple question 

```{python}
#| echo: true
simple_queries_df = (sqlite_df
  .pipe(lambda df: df[df.db_id == 'california_schools'])
  .pipe(lambda df: df[df.difficulty == 'simple']))

simple_queries_df.head(2).set_index("db_id")
```

---

### Getting one simple question 

```{python}
#| echo: false
import pandas as pd
pd.set_option('display.max_colwidth', 0)
```

```{python}
#| echo: true


question_sql_df = (simple_queries_df
  .pipe(lambda df: df[["question", "SQL"]])
  .reset_index())
```

::: {.fragment .fade-left}
Let's take a look at the `question` and `SQL` column

```{python}
#| echo: true
#| output-location: fragment

question, query = question_sql_df.loc[
  0, 
  ["question", "SQL"]]
display(question, query)
```

:::

::: notes
This is one of the valid SQL queries for this question
There may be other. When we're comparing performance, 
in situations where we have multiple rows and there
are no aggregations we need to decide wether the order is important or not
:::

---

### Execute the queries {.smaller}

::: {style="font-size: 60%;"}
Now we run the `SQL` column captured in the variable `query` through SQLAlchemy and
plot the results as a DataFrame
:::

```{python}
#| echo: true
#| output-location: fragment

from sqlalchemy import text
with engine.connect() as conn:
    result = conn.execute(text(query))
    res_df = pd.DataFrame(result.fetchall()) # üêº ‚ú®
    display(res_df)
```




::: footer
:::


---

# Convert natural language questions into SQL {.smaller}

:::: {.columns}
::: {.column}
::: {.fragment .fade-left}
LLMs are quite capable of writing functional SQL queries, from the `code` or `coder`
ones, to specific ones for SQL generation
:::

:::

::: {.column}

::: {.fragment .fade-left}
For example, some IBM trained models include:

- granite-20b-code-instruct
- granite-34b-code-instruct
- granite-20b-code-base-schema-linking
- granite-20b-code-base-sql-gen

[More info on these models {{< bi box-arrow-up-right >}}](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-models-ibm.html?context=wx&locale=en&audience=wdp#granite-code-instruct-models)
[And also in ü§ó {{< bi box-arrow-up-right >}}](https://huggingface.co/models?search=sql)

:::

:::

::::

::: notes
For those of us who have been writing code for some time and 
fell in love with ORMs when they were the *hot* new thing, LLMs
can take us to the next level!
:::

---

## Prompts for SQL generation {transition="convex"}


::: {.fragment .fade-in-then-semi-out}
LLMs don't know the  üèóÔ∏è structure of our database, and may hallucinate 
about it, or create some flat out invalid SQL
:::

::: {.fragment .fade-in-then-semi-out}
We have to  provide __extra__ information about the structure in the instructions

For this we will use a **prompt** string with some palace-holders {{< bi braces >}}.
:::

::: {.fragment .fade-in-then-semi-out .small}
Some research papers from our team from our team:

[{{< bi filetype-pdf >}} Weakly Supervised Detection of Hallucinations in LLMs](https://arxiv.org/pdf/2312.02798)
:::

::: {.fragment .fade-in-then-semi-out .small}
[{{< bi filetype-pdf >}} Localizing Persona Representations In LLMs](https://arxiv.org/pdf/2505.24539)
:::

::: notes
Now we're gonna see what we're put in the prompt, for starters the schema
:::

---


### Getting the schema for the prompt

LangChain (ü¶ú ‚õìÔ∏è) community üêç üì¶ provides a simple class that can retrieve some schema information [^qa_lc].

It implements the best practices as specified in: [{{< bi filetype-pdf >}} Rajkumar et al, 2022](https://arxiv.org/abs/2204.00498)

```{python}
#| echo: true
!uv pip install langchain-community
```

```{python}
#| echo: true

from langchain_community.utilities import SQLDatabase
db = SQLDatabase(engine=engine)

display(db.get_usable_table_names())
```

[^qa_lc]: [SQL Question Answering](https://python.langchain.com/docs/tutorials/sql_qa/#system-prompt){target="_blank"}

::: notes
As we can see, the table names may not be immediately understandable ü§î
:::



---

###

```{python}
#| echo: true
#| code-overflow: wrap
#| 
print(db.get_table_info())


```

<!-- Disable footer -->
::: footer
:::


---

### Prompts {.smaller}

```{python}
#| echo: true
#| code-line-numbers: 1-7|8-14|15-18

system_message = """
Given an input question, create a syntactically correct {dialect} query to
run to help find the answer. Unless the user specifies in his question a
specific number of examples they wish to obtain, always limit your query to
at most {top_k} results. You can order the results by a relevant column to
return the most interesting examples in the database.

Never query for all the columns from a specific table, only ask for a the
few relevant columns given the question.

Pay attention to use only the column names that you can see in the schema
description. Be careful to not query for columns that do not exist. Also,
pay attention to which column is in which table.

Only use the following tables:
{table_info}
"""
```

::: {.small_text}

[source {{< bi box-arrow-up-right >}}](https://python.langchain.com/docs/tutorials/sql_qa/#convert-question-to-sql-query)

:::

--- 

## The messages that constitute the prompt {.smaller}

Now we construct a list of messages. These are `dicts` which have a key
`user` or `system`, and a `content`.

```{python}
#| echo: true
#| output-location: slide
#| code-line-numbers: "4-10|11-14|19-23"
def generate_messages(question, dialect="SQL", top_k=5, table_info=""):
    # Create a ChatPromptTemplate
    messages = [
      {
        "role": "system", 
        "content": system_message.format(
          dialect=dialect, 
          top_k=top_k, 
          table_info=table_info
        )},
      {
        "role": "user", 
        "content": question
      }
    ]
    
    return messages

messages = generate_messages(
  question=question, 
  dialect=db.dialect, top_k=10, 
  table_info=db.get_table_info())
messages
```

```{python}
#| echo: false
import json
with open("messages.json", "w") as fp:
    json.dump(messages, fp)
```

--- 

### üîé the `system` message

```{python}
#| echo: false
#| output-location: default
#| classes: scrollable 
print(messages[0]['content'])
```

::: footer
:::

---

## Calling the LLM with the prompt {.smaller}

`litellm` is a Open Source Python library (and also a Proxy) that can be
used to run inference and compute embeddings with a vast number
of providers (new models added weekly).

Among the provider lists we have:

::: {.incremental}

- Ollama / LMStudio / llama.cpp
- IBM WatsonX.ai
- Anthropic
- OpenAI
- AWS Sagemaker
- OpenRouter
- [and more {{< bi info-circle >}}](https://docs.litellm.ai/docs/providers)
:::

---

### Using litellm

::: {.fragment .fade-in-then-semi-out}
```{python}
#| echo: true
!uv add litellm --quiet
```
:::

<!-- [^extras]:   
  `caching` , `extra-proxy` , `mlflow` , `proxy` , `semantic-router`, `utils` -->


::: {.fragment .fade-in}
To run inference, we just call the [`completions`](https://docs.litellm.ai/docs/completion/input) module function:

<!-- Enable disk cachesq -->

```{python}
#| echo: false
# Speed up execution
import litellm 

from litellm.caching.caching import Cache
litellm.cache = Cache(type="disk")
# Remove debug messages
litellm.suppress_debug_info = True
```

```{python}
#| echo: true

import litellm

# model = "ollama_chat/granite-code:20b"
model = "watsonx/ibm/granite-3-2-8b-instruct"

response = litellm.completion(
  model=model,
  messages=messages,
)

```

:::


::: notes

There are some reasonably good LLMs under the coder and instruct in Hugging Face
Some of these can be run locally with some inference server like `Ollama`, 
`llama.cpp` or `LMStudio`, and also use pubic 
ones

:::

---

```{python}
#| echo: true
#| classes: scrollable
print(response)
```
---

## Structured output {.smaller}

::: {.fragment}
Although it looks we have the SQL, LLMs currently support structured output
Also known as `JSON Schema`, this allows us to also retrieve some explanation

A Python library that generates JSON Schema from classes automatically 
is called Pydantic. Let's see how we can ask 2 things from the LLM
:::

::: {.fragment}
```{python}
#| echo: true
#| code-line-numbers: "1|2-5|9-13|14-17"

from pydantic import BaseModel, Field
class SQLOutput(BaseModel):
    sql: str = Field(description="The SQL query")
    explanation: str = Field(
      description="The reasoning for the query construction")

# Optional 
# litellm.enable_json_schema_validation = True

response = litellm.completion(
  model=model,
  messages=messages,
  response_format=SQLOutput,
)
output = SQLOutput.model_validate_json(
  response.choices[0].message.content)
```

:::
---

### Attributes of `output` object


```{python}
#| echo: true
output.sql
```

::: {.fragment}

```{python}
#| echo: true
output.explanation
```
:::



--- 

### LLM generated SQL vs dataset query
```{python}
#| echo: true
display(query)
display(output.sql)
```

Execution result: 

```{python}
try:
  with engine.connect() as c:
    response = c.execute(text(output.sql))
    print(response.fetchall())
except Exception as err:
  print(err)
```

---

## Improving the Generation with more context

The BIRD mini has some CSV files that help understanding
the tables values and relationships

Let's pass this to the prompt generation system message...

#### TODO
```{python}
#| echo: false
# from cheap_repr import cheap_repr
# messages.insert(1, {
#   "role": ""
# })
# cheap_repr(messages)
```

---

# Create a workflow

::: notes

:::

---

# Chaining generation and execution

::: {.fragment .fade-in-then-semi-out}
<img src="./img/langgraph.svg" height="40px" style="float: left; padding-top: .5em; padding-right: .25em"/> 
<p>`LangGraph` allows us to create state machines around LLM calls, using
functions as nodes.</p>
:::

::: {.fragment .fade-in-then-semi-out}
We will now put the code we saw into node functions
:::

::: {.fragment .fade-in-then-semi-out}
We will have a `Input`, and `Output` a shared `State` and some global configuration, 
instead of global variables
:::

---

![](./img/LLMsSQL-inject.svg)

---

## Graph creation

```{.python include="src/sql_with_llms/main.py" snippet="graph" dedent="4" code-line-numbers="false"}
```

::: {.fragment}
Let's take a look at how we define those function and the data types
they receive and return
:::


::: notes
1. In Langraph the node functions must be defined before, but we're presenting this
   before for clarity
:::
---

```{python}
#| echo: false
#| output: true

from  sql_with_llms.main import create_graph
from IPython.display import Markdown
graph = create_graph()
display(Markdown("```{mermaid}\n%s\n```" % graph.get_graph().draw_mermaid()))
```

---

## Input class

This is what we expect to be passed to our state machine 

```{.python include="src/sql_with_llms/main.py" snippet="input"}
```

---

## State class

This is what the state machine will pass to the nodes (functions)
and expect from them (except the first and last node which use `Input`/`Output`)

```{.python include="src/sql_with_llms/main.py" snippet="state" code-line-numbers="false"}
```

---

## Output class

This is the response back to the user, we will use it in the *last* node 
```{.python include="src/sql_with_llms/main.py" snippet="output" code-line-numbers="false"}
```

---

## The init node function

This node consumes the input converting the something of shape `Input` into
a `State`. The next node will be prompt generation, so we will be save the
`question`.

```{.python include="src/sql_with_llms/main.py" snippet="init" code-line-numbers="false"}
```

---

## Global configuration class

The nodes beyond the initial one will require to have a bit more information
that won't change (not considered state), for this we will define a global 
```{.python include="src/sql_with_llms/main.py" snippet="cfg" code-line-numbers="false"}
```

---

## Prompt generation function node

```{.python include="src/sql_with_llms/main.py" snippet="prompt_gen" code-line-numbers="false"}
```

---


![](./img/LLMsSQL-workflow.svg)

---

## Accommodating global configuration

```{.python include="src/sql_with_llms/main.py" snippet="ext_lg" code-line-numbers="false"}
```

---

## LLM calling function node


```{.python include="src/sql_with_llms/main.py" snippet="call_llm" code-line-numbers="false"}
```


## SQL Execution function node

```{.python include="src/sql_with_llms/main.py" snippet="exec_sql" code-line-numbers="false"}
```

---

# Lessons learned

---

## Move from function to classes

::: {style="font-size: 80%;"}
::::: {.fragment}
- The workflow state can be a `dict` or `TypedDict`, but `dataclass` or a `BaseModel` is 
  more convenient
```{.python}
class State(TypedDict):
  connection: ...
  messages: ...
  schema: ...
```
:::::
::::: {.fragment}
:::: {.columns}
::: {.column width="48%"}
::::: {.fragment}

- A simple function that works with the state
```{.python}
def create_sql(state: State) -> State:
  # Do something
  return State(...)
```
:::::
:::

::: {.column width="4%"}
<!-- Gap -->
:::
::: {.column width="48%"}

::::: {.fragment}
- This can also be a class if we want to pass more information

```{.python}
class CreateSQLNode:
  def __init__(self, config: ...) -> None:
    self.config = config
  
  def __call__(
    self, state: State
  ) -> State:
    # do something
    return State(...)
```
:::::

:::
::::
:::::

:::


---

## SQL validation

- We can pass the SQL to SQLAlchemy engine to validate if the syntax is correct
- SQLGlot `parse_one()` provides asn AST of the SQL for:
  - Finding dangerous operations (DML)
  - For any updates to the query, working on an AST is better than string substitution

---

## Enhancing the context

- Tables with a high number of columns can make the context larger
  - RAG at the column level is a common technique to improve efficiency and accuracy
- Retrieving the values from the DB can help the LLM produce better queries
- Providing some examples in the context with the `question` and `SQL` pairs
  also improves efficiency

---

## Dynamic context

- Other types of examples of well understood terms may require 
  some mapping
  
  - For example, the term Q1 can mean, *quarter one*, but it should
    be supplied dynamically

- Function nodes, when we want to modify the query another approach
  is to create

---

## Extensibility

::: {.sequential}
- Offload node responsibilities into external services (e.g. DB execution) 
- Pipelines defined as YAML
- Each node is registered in a collection of nodes, and each
  node in the pipeline accepts the `__init__` arguments
- Use Python's built in `entry-point` system to *install* nodes
  This is a good approach to separate production from 
:::

---


# Thank you

::: {.fragment}
### Questions
:::

::: {.fragment}
#### Contact
:::
