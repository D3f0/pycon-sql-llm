---
title: "üí¨ ‚ùì ü§ñ Asking questions to your database with LLMs "
# subtitle: "Techniques for effective SQL generation"
author: Nahuel Defoss√© <br>[nahuel.defosse@ibm.com](mailto:nahuel.defosse@ibm.com)<br>IBM Research Kenya Lab
date: September, 2025
embed-resources: false
# https://quarto.org/docs/computations/caching.html
execute: 
  cache: true
  keep-ipynb: true
format:
  revealjs:
    toc: true
    toc-depth: 1
    slide-number: true
    transition: slide
    code-copy: true
    highlight-style: github
    theme: [solarized, custom.scss]  
  
notes: >
  This talk will cover a research project within IBM Research focused on generating structured 
  query language queries for your data using open-source models based on the work of the 
  Nairobi and Johannesbug Research labs.

  We will cover the state of the art in terms of techniques to augment the prompt in terms of 
  database structure and contents to help build more specific queries, error correction, 
  prompt reformulation, and other approaches. We will briefly discuss how 
  fine-tuning of models can help in some scenarios.

  We will close this talk by discussing the tools used for building this kind 
  of project and some of the lessons learned along the way.

todo: >
  https://research.ibm.com/projects/flowpilot
  Add https://www.youtube.com/watch?v=bEgnVKb_J4k

---

# About myself {.smaller}

:::: {.columns}
::: {.column width="40%"}
![](./img/memes/myself.gif){height="12em"}
:::

::: {.column width="60%"}

::: {.incremental}
- üêç Pythonista with 18 years of experience.
  - Co-organized SciPy Latin America
- üöú Worked as CTO in  Hello Tractor 
- üß™ Software Engineer at IBM Research 
- üõ∞Ô∏è Worked in Foundational Models for Geospatial applications
- üí¨ Currently working on [Flowpilot {{< bi link-45deg >}}](https://research.ibm.com/projects/flowpilot), providing core features to 
  different products and divisions.
::: 

:::
::::


---

## Follow along (or at üè°) {.center}

{{< qrcode https://jmbuhr.de >}}


---

# Getting started

---

## About LLMs

::: {.fragment .fade-in-then-semi-out}
LLMs are quite capable of writing functional SQL queries.
:::


::: {.fragment .fade-in-then-semi-out}
For those of us who have been writing code for some time and 
fell in love with ORMs when they were the *hot* new thing, LLMs
can take us to the next level!
:::

::: {.fragment .fade-in-then-semi-out}
But they don't know the üèóÔ∏è structure of our database, and may hallucinate 
about it, or create some flat out invalid SQL, so we will need to handle
some of this scenarios.
:::

::: {.fragment .fade-in-then-semi-out}
Some research papers from our team from our team:

[{{< bi filetype-pdf >}} Weakly Supervised Detection of Hallucinations in LLMs](https://arxiv.org/pdf/2312.02798)

[{{< bi filetype-pdf >}} Localizing Persona Representations In LLMs](https://arxiv.org/pdf/2505.24539)
:::

::: notes
In Flowpilot we created a framework inspired on LangGraph for this.
:::

---

## LLMs for Code Generation ‚úçÔ∏è
::: {.fragment .fade-in-then-semi-out}
There are some reasonably good LLMs under the coder and instruct in Hugging Face.
Some of these can be run locally with some inference server like `Ollama`, 
`llama.cpp` or `LMStudio`, and also use pubic 
ones.
:::

::: {.fragment .fade-in-then-semi-out}
Running LLMs locally or on-prem scenarios is quite important when
we need to preserve the confidentiality of the data. 
:::

::: {.fragment .fade-in-then-semi-out}
We'll take a look at a library/proxy that can help us move between LLMs
called `litellm` (can also add observability, cost tracking, etc.).
:::

---

### Contacting LLMs with `littellm` SDK

::: {.fragment .fade-in-then-semi-out}
`uv add littellm` and, for local execution, we can also add ollama Python package `uv add ollama`.
:::

::: {.fragment .fade-in-then-semi-out}
`litellm` uses a prefix for the provider of the LLM (it supports 100+ providers) 
[{{< bi question-octagon >}}](Ãåhttps://docs.litellm.ai/docs/set_keys)
:::

::: {.fragment .fade-in-then-semi-out}
Quick example of how to use it...

```python
import litellm

response = litellm.completion(
            model="ollama/granite-code:20b",
            messages=[{"role": "user", "content": "Hey, how's it going?"}],
        )
print(response)
```
:::

---

### Example response

::: {.fragment .fade-in-then-semi-out}
```python
ModelResponse(
  id='chatcmpl-c34aae0e-a299-42a0-922e-aabd96e3a91e', 
  created=1758269174, 
  model='ollama/granite-code:20b', 
  object='chat.completion', 
  system_fingerprint=None, 
  choices=[Choices(
    finish_reason='stop', 
    index=0, 
    message=Message(
      content='### Bot:\nHey there! How can I assist you today?\n\n', 
      role='assistant', 
      tool_calls=None, 
      function_call=None, 
      provider_specific_fields=None
      )
    )], 
    usage=Usage(
      completion_tokens=17, 
      prompt_tokens=20, 
      total_tokens=37, 
      completion_tokens_details=None, 
      prompt_tokens_details=None
      )
    )
```
:::



---

## Structure and Configuration

::: {.fragment .fade-in-then-semi-out}
For simplicity we hardcoded the model to ü¶ô model `ollama/granite-code:20b`. Let's take a look
at how we can make this more flexible with a configuration class.
:::

::: {.fragment .fade-in-then-semi-out}
We're going to use Pydantic configuration system (`uv add pydantic-settings`), whihc
will serve us as a `load_dotenv()` on steroids {{< bi hurricane >}}
::: 

::: {.fragment .fade-in-then-semi-out}
Our goal is to have most of the configuration defined either in environment variables
or also loadable from a our popular `.env`.
:::

---

### Configuration with Pydantic Settings

```{python}
#| echo: true
from pydantic_settings import BaseSettings


```

---

### Getting some completions (cont.) {.center}
```python
#| echo: true
#| output: true
#| 
from litellm import completion

messages = [
  {"role": "user", 
  "content": "Create a small customer table in SQL"}]
response = completion(
  model="ollama/hf.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF:F16", 
  messages=messages, stream=False
)
print(response['choices'][0]['message']['content'])
```

---

## Workflows 

::: {.fragment .fade-in-then-semi-out}
When our code starts to become larger than a script can handle üèãüèæ‚Äç‚ôÄÔ∏è, `LangGraph` is a great
tool to provide some reusable organization. We can add it with `uv add langgraph`.
:::

::: {.fragment .fade-in-then-semi-out}
This will help us to separate some concerns like  üìê *validation* , üèÉüèΩ *execution* and later,  ‚ò¢Ô∏è *enrichment*.
:::

::: {.fragment .fade-in-then-semi-out .center}
Let's see how we can create something like this:
```{mermaid}
%%| fig-align: center
flowchart LR
    S[Start]
    CallLLM[Call litellm's completion]
    S -->|State: Dict| CallLLM
    CallLLM --> End
    
```
:::

---

## Workflows - LangGraph
```python
#| code-overflow: wrap

from typing import Dict, Any, List
from pydantic import BaseModel, Field
from langgraph.graph import StateGraph
from litellm import completion
import os

# Configuration using Pydantic
class ModelConfig(BaseModel):
    model_name: str = Field(default="gpt-3.5-turbo")
    temperature: float = Field(default=0.7)
    max_tokens: int = Field(default=500)

# Global configuration object
config = ModelConfig()

# State definition
class State(BaseModel):
    input: str
    output: str = ""

# Node function to call the LLM
def call_llm(state: State) -> State:
    response = completion(
        model=config.model_name,
        messages=[{"role": "user", "content": state.input}],
        temperature=config.temperature,
        max_tokens=config.max_tokens
    )
    
    state.output = response.choices[0].message.content
    return state

# Create the graph
def create_workflow() -> StateGraph:
    # Define the graph
    workflow = StateGraph(State)
    
    # Add nodes
    workflow.add_node("call_llm", call_llm)
    
    # Define the edges
    workflow.set_entry_point("call_llm")
    
    # Compile the graph
    return workflow.compile()

# # Example usage
# if __name__ == "__main__":
#     # Configure the model
#     config.model_name = "gpt-4"
#     config.temperature = 0.5
    
#     # Create and run the workflow
#     graph = create_workflow()
#     result = graph.invoke({"input": "Explain quantum computing in simple terms."})
#     print(result.output)
```
---

<!-- ### More tools ‚öíÔ∏è for dealing with SQL {background-image="./img/memes/old-man-giving-link-a-sword-in-the-legend-of-zelda.jpg"} -->
### More tools ‚öíÔ∏è for dealing with SQL {background-image="https://static0.thegamerimages.com/wordpress/wp-content/uploads/2023/04/old-man-giving-link-a-sword-in-the-legend-of-zelda.jpg?q=50&fit=crop&w=1100&h=618&dpr=1.5"}

::: {.columns .small_text}
::: {.column }
#### LangGraph

Allows us to create code **Workflows** with nodes and edges.
Nodes can be function or classes.
:::

::: {.column}
#### Pydantic

**Data validation** library, allows to convert JSON into
Python objects easily and has very good performance given
that it's been re-written in Rust ü¶Ä
:::

:::


::: {.columns .small_text}
::: {.column }
#### SQLGlot

A sql **parser** and transpiler. Generates an AST for multiple 
[31+ database engines](https://sqlglot.com/sqlglot/dialects.html) that can be converted from one format to another.
:::

::: {.column}
#### SQLAlchemy

Can connect to multiple databases and execute queries. 
Has a great inspection API which can give us back
the structure of the DB.
:::

:::

---

## Let's go for Bobby tables {.center}

![](./img/memes/exploits_of_a_mom.png)

::: footer
[`xkcd 327`](https://www.explainxkcd.com/wiki/index.php/327:_Exploits_of_a_Mom)
::: 

---

## Let's grab some {{< bi database >}} from üê¶ benchmark

::: {.smaller}
```{python}
#| echo: true
import fsspec, rich
url = "http://bird-bench.oss-cn-beijing.aliyuncs.com/dev.zip"
rfs, *_ = fsspec.mapping.url_to_fs(f"filecache::zip::{url}", mode="r")

inner_zip_path, *_ = rfs.glob('**/dev_databases.zip')
rich.print(inner_zip_path)
#db_zips = rfs.open(inner_zip_path)



```
:::

---
