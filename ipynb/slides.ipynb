{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: 'üí¨ {{< bi database >}} ü§ñ Asking questions to your database with LLMs '\n",
    "author: Nahuel Defoss√© {{< bi envelope >}} [nahuel.defosse@ibm.com](mailto:nahuel.defosse@ibm.com)<br>IBM Research Africa\n",
    "embed-resources: false\n",
    "footer: '*Asking question to {{< bi database >}} with LLMs* - üêç PyCon üá∞üá™ 2025'\n",
    "execute:\n",
    "  cache: true\n",
    "  keep-ipynb: true\n",
    "format:\n",
    "  revealjs:\n",
    "    toc: false\n",
    "    toc-depth: 1\n",
    "    slide-number: true\n",
    "    transition: slide\n",
    "    code-copy: true\n",
    "    highlight-style: a11y\n",
    "    code-overflow: wrap\n",
    "    theme:\n",
    "      - solarized\n",
    "      - custom.scss\n",
    "    header-logo-right: ./img/logo.png\n",
    "    header-logo-right-size: 2em\n",
    "    preview-links: auto\n",
    "filters:\n",
    "  - reveal-logo\n",
    "  - include-code-files\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"JUPYTER_COLUMNS\"] = \"80\"\n",
    "# %load_ext rich"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About myself {.smaller}\n",
    "\n",
    ":::: {.columns}\n",
    "::: {.column width=\"40%\"}\n",
    ":::: {.content-visible when-format=\"html\"}\n",
    "![](./img/memes/myself.gif){height=\"12em\"}\n",
    "::::\n",
    ":::\n",
    "\n",
    "::: {.column width=\"60%\"}\n",
    "\n",
    "::: {.incremental}\n",
    "- üêç Pythonista with 18 years of experience üá¶üá∑ üßâ\n",
    "  - Co-organized SciPy Latin America\n",
    "  - PyCon Argentina and EuroPython speaker\n",
    "- üöú Worked as CTO in  Hello Tractor \n",
    "- üß™ Software Engineer at IBM Research \n",
    "- üõ∞Ô∏è Worked in [Foundational Models for Geospatial applications {{< bi link >}}](https://www.earthdata.nasa.gov/news/nasa-ibm-openly-release-geospatial-ai-foundation-model-nasa-earth-observation-data)\n",
    "- üí¨ Currently working on [Flowpilot {{< bi link >}}](https://research.ibm.com/projects/flowpilot), providing core features to \n",
    "  different products and divisions\n",
    "::: \n",
    "\n",
    ":::\n",
    "::::\n",
    "\n",
    "---\n",
    "\n",
    "::: {.content-visible when-format=\"html\"}\n",
    "\n",
    "## Tomorrow {.center}\n",
    "\n",
    "::: {.columns}\n",
    "::: {.column width=\"30%\"}\n",
    "![](./img/promo/wanjiru.jpg)\n",
    ":::\n",
    "::: {.column width=\"30%\"}\n",
    "![](./img/promo/beldine.jpg)\n",
    ":::\n",
    "::: {.column width=\"30%\"}\n",
    "![](./img/promo/reggie.png)\n",
    ":::\n",
    "::: \n",
    "\n",
    "Learn about [Multimodal Geospatial Foundation Models with Terratorch](https://pycon-kenya-2025.sessionize.com/session/954933){target=\"_blank\"}\n",
    " by Wanjiru, Beldine and Reggie\n",
    "\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "::: {.content-visible when-format=\"html\"}\n",
    "\n",
    "## Follow along (or at üè°) {.center}\n",
    "\n",
    "::: {.columns}\n",
    "::: {.column}\n",
    "::: {style=\"text-align: center;\"}\n",
    "<a href=\"https://d3f0.github.io/pycon-sql-llm/slides.html#/title-slide\" target=\"_blank\">\n",
    "\n",
    "{{< qrcode https://d3f0.github.io/pycon-sql-llm/slides.html#/title-slide qr1 width=400 height=400 colorDark='#0011bb' >}}\n",
    "\n",
    "</a>\n",
    "\n",
    "Get the slides {{< bi arrow-up-circle >}}\n",
    ":::\n",
    "\n",
    ":::\n",
    "::: {.column}\n",
    "::: {style=\"font-size: .67em\"}\n",
    "Get the code {{< bi arrow-down-circle >}}\n",
    "\n",
    "[{{< bi github >}} https://github.com/D3f0/pycon-sql-llm](https://github.com/D3f0/pycon-sql-llm)\n",
    ":::\n",
    ":::\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "# Agenda\n",
    "\n",
    "In this talk we're gonna show how to use Python to:\n",
    "\n",
    "  - {{< bi database >}} Connect to a database and execute the queries\n",
    "  - {{< bi filetype-sql >}} Convert natural language questions into SQL\n",
    "  - {{< bi diagram-3 >}} Create a workflow\n",
    "  - {{< bi database-gear >}} Managing configuration\n",
    "  - {{< bi bookmark-check >}} Lessons learned\n",
    "\n",
    "---\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "![](./img/LLMsSQL.svg)\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "# DB Inspection\n",
    "<!-- ## Connect to a database and execute the queries -->\n",
    "\n",
    "---\n",
    "\n",
    "### Public datasets used in text to SQL\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.column}\n",
    "::: {.fragment .fade-left .small_text}\n",
    "These datasets define:\n",
    "\n",
    "- ‚ùì Natural language questions\n",
    "- ü§ûüèΩ Expected SQL\n",
    "- üèóÔ∏è Database schema & content\n",
    "- üîé Evaluation metrics \n",
    "- ü•á Leaderboard\n",
    ":::\n",
    ":::\n",
    "\n",
    "::: {.column}\n",
    "\n",
    "::: {.fragment .fade-right}\n",
    "- [üï∑Ô∏è Spider](https://yale-lily.github.io/spider){target=\"_bank\"}\n",
    ":::\n",
    "::: {.fragment .fade-right}\n",
    "- [üï∑Ô∏è üï∑Ô∏è Spider 2](https://spider2-sql.github.io/){target=\"_bank\"}\n",
    ":::\n",
    "::: {.fragment .fade-right}\n",
    "- üê¶ [BIRD](https://bird-bench.github.io/){target=\"_bank\"}\n",
    ":::\n",
    "::: {.fragment .fade-right}\n",
    "- [üèπ Archer](https://sig4kg.github.io/archer-bench/){target=\"_bank\"}\n",
    ":::\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "\n",
    "::: notes\n",
    "There are various academic datasets that are used for advancing the field\n",
    "of text2sql.\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "## BIRD \n",
    "\n",
    "![[IBM Research in the leaderboard last year](https://research.ibm.com/blog/granite-LLM-text-to-SQL)](./img/ibm-granite-leaderboard.png){height=\"400px\" fig-align=\"center\"}\n",
    "\n",
    "\n",
    "::: notes\n",
    "We're select BIRD dataset since we have some experience with it, we managed\n",
    "to get to the top of the leatherboard in 2024/6\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "### BIRD mini-dev\n",
    "\n",
    "- It consist of 500 queries classified as **simple**, **moderate** and **challenging** [{{< bi box-arrow-up-right >}}](https://github.com/bird-bench/mini_dev)\n",
    "\n",
    "::: {.fragment .fade-left}\n",
    "\n",
    "<!-- Setup cell without output with imports -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.console import Console\n",
    "\n",
    "_c = Console(width=45)\n",
    "# Ensure print are contained within the range\n",
    "print = _c.print\n",
    "\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "output-location": "slide"
   },
   "outputs": [],
   "source": [
    "# | echo: true\n",
    "from datasets import load_dataset, DownloadConfig\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\n",
    "    \"birdsql/bird_mini_dev\", download_config=DownloadConfig(disable_tqdm=True)\n",
    ")\n",
    "\n",
    "# Only SQLite (there's Postgres and MySQL in the same dataset)\n",
    "sqlite_df = (\n",
    "    dataset[\"mini_dev_sqlite\"]\n",
    "    .to_pandas()\n",
    "    .pipe(lambda df: df.drop(columns=[\"question_id\"]))\n",
    ")\n",
    "display(sqlite_df.sample(5, random_state=17))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "### Downloading BIRD databases\n",
    "\n",
    "<!-- - We will be using [`uvx`](https://docs.astral.sh/uv/concepts/tools/#tool-versions)[^uvx] with the `gdown` package as follows: -->\n",
    "\n",
    "```shell\n",
    "uvx gdown https://drive.google.com/file/d/13VLWIwpw5E3d5DUkMvzw7hvHE67a4XkG/\n",
    "```\n",
    "![](./img/download_minidev.png)\n",
    "<!-- Or just download from the [ {{< bi google >}} {{< bi hdd >}} link](https://drive.google.com/file/d/13VLWIwpw5E3d5DUkMvzw7hvHE67a4XkG/view?usp=sharing)  -->\n",
    "\n",
    "Extracting the archive (3.3GiB)\n",
    "\n",
    "```shell\n",
    "unzip minidev_703.zip\n",
    "```\n",
    "<!-- [^uvx]: It comes as part of `uv`, it's a shorthand for `uv tool run` -->\n",
    "\n",
    "::: footer\n",
    "The dataset doesn't contain the SQL flies, these are shared separately as zipfiles. \n",
    "In this case it's located in Google Drive.\n",
    ":::\n",
    "---\n",
    "\n",
    "### Picking the example database `california_schools`\n",
    "\n",
    "In `minidev/MINIDEV/dev_databases/california_schools/` we find the {{< bi database >}} {{< bi file >}}\n",
    "\n",
    "![](./img/california_school.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: true\n",
    "# | code-overflow: wrap\n",
    "from pathlib import Path\n",
    "\n",
    "base = Path(\"./minidev/MINIDEV/dev_databases/california_schools/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ER of `california_schools` {{< bi database >}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from eralchemy import render_er\n",
    "# render_er(db_url, './img/output/erd_from_sqlite.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/er-california-schools.png){.lightbox}\n",
    "\n",
    "::: footer\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Creating the `Engine`\n",
    "\n",
    "Creating an `Engine` instance connected to a SQLite database to run our queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: true\n",
    "# | code-overflow: wrap\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "db_path = base / \"california_schools.sqlite\"\n",
    "db_url = f\"sqlite:///{db_path}\"\n",
    "engine = create_engine(db_url)\n",
    "engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: notes\n",
    "This allows us to execute, manage transactions and do database introspection.\n",
    ":::\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Getting one simple question "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: true\n",
    "simple_queries_df = sqlite_df.pipe(\n",
    "    lambda df: df[df.db_id == \"california_schools\"]\n",
    ").pipe(lambda df: df[df.difficulty == \"simple\"])\n",
    "\n",
    "simple_queries_df.head(2).set_index(\"db_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Getting a `simple` question and query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: true\n",
    "\n",
    "\n",
    "question_sql_df = simple_queries_df.pipe(\n",
    "    lambda df: df[[\"question\", \"SQL\"]]\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.fragment .fade-left}\n",
    "Let's take a look at the `question` and `SQL` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "output-location": "fragment"
   },
   "outputs": [],
   "source": [
    "# | echo: true\n",
    "\n",
    "question, query = question_sql_df.loc[0, [\"question\", \"SQL\"]]\n",
    "display(question, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::\n",
    "\n",
    "::: notes\n",
    "This is one of the valid SQL queries for this question\n",
    "There may be other. When we're comparing performance, \n",
    "in situations where we have multiple rows and there\n",
    "are no aggregations we need to decide wether the order is important or not\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "### Execute the query {.smaller}\n",
    "\n",
    "::: {style=\"font-size: 60%;\"}\n",
    "Now we run the `SQL` column captured in the variable `query` through SQLAlchemy and\n",
    "plot the results as a DataFrame\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "output-location": "fragment"
   },
   "outputs": [],
   "source": [
    "# | echo: true\n",
    "\n",
    "from sqlalchemy import text\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(query))\n",
    "    res_df = pd.DataFrame(result.fetchall())  # üêº ‚ú®\n",
    "    display(res_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: footer\n",
    ":::\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "# Convert natural language questions into SQL {.smaller}\n",
    "\n",
    ":::: {.columns}\n",
    "::: {.column}\n",
    "::: {.fragment .fade-left}\n",
    "LLMs are quite capable of writing functional SQL queries, from the `code` or `coder`\n",
    "ones, to specific ones for SQL generation\n",
    ":::\n",
    "\n",
    ":::\n",
    "\n",
    "::: {.column}\n",
    "\n",
    "::: {.fragment .fade-left}\n",
    "For example, some IBM trained models include:\n",
    "\n",
    "- granite-3-2-8b-instruct\n",
    "- granite-34b-code-instruct\n",
    "- granite-20b-code-base-schema-linking\n",
    "- granite-20b-code-base-sql-gen\n",
    "\n",
    "[More info on these models {{< bi box-arrow-up-right >}}](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-models-ibm.html?context=wx&locale=en&audience=wdp#granite-code-instruct-models) | \n",
    "[And also in ü§ó {{< bi box-arrow-up-right >}}](https://huggingface.co/models?search=sql)\n",
    "\n",
    ":::\n",
    "\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "::: notes\n",
    "For those of us who have been writing code for some time and \n",
    "fell in love with ORMs when they were the *hot* new thing, LLMs\n",
    "can take us to the next level!\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "## Prompts for SQL generation {transition=\"convex\"}\n",
    "\n",
    "\n",
    "::: {.fragment .fade-in-then-semi-out}\n",
    "LLMs don't know the  üèóÔ∏è structure of our database, and may hallucinate \n",
    "about it, or create some flat out invalid SQL\n",
    ":::\n",
    "\n",
    "::: {.fragment .fade-in-then-semi-out}\n",
    "We have to  provide __extra__ information about the structure in the instructions\n",
    "\n",
    "For this we will use a **prompt** string with some palace-holders {{< bi braces >}}.\n",
    ":::\n",
    "\n",
    "::: {.fragment .fade-in-then-semi-out .small}\n",
    "Some research papers from our team from our team:\n",
    "\n",
    "[{{< bi filetype-pdf >}} Weakly Supervised Detection of Hallucinations in LLMs](https://arxiv.org/pdf/2312.02798)\n",
    ":::\n",
    "\n",
    "::: {.fragment .fade-in-then-semi-out .small}\n",
    "[{{< bi filetype-pdf >}} Localizing Persona Representations In LLMs](https://arxiv.org/pdf/2505.24539)\n",
    ":::\n",
    "\n",
    "::: notes\n",
    "Now we're gonna see what we're put in the prompt, for starters the schema\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Getting the schema for the prompt\n",
    "\n",
    "LangChain (ü¶ú ‚õìÔ∏è) community üêç üì¶ provides a simple class that can retrieve some schema information\n",
    "[SQL Question Answering](https://python.langchain.com/docs/tutorials/sql_qa/#system-prompt){target=\"_blank\"}\n",
    "\n",
    "::: {.small}\n",
    "It implements the best practices as specified in: [{{< bi filetype-pdf >}} Rajkumar et al, 2022](https://arxiv.org/abs/2204.00498)\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: true\n",
    "!uv pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: true\n",
    "\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "\n",
    "db = SQLDatabase(engine=engine)\n",
    "\n",
    "display(db.get_usable_table_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: notes\n",
    "As we can see, the table names may not be immediately understandable ü§î\n",
    ":::\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: true\n",
    "# | code-overflow: wrap\n",
    "# |\n",
    "print(db.get_table_info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Disable footer -->\n",
    "::: footer\n",
    ":::\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Prompts {.smaller}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: true\n",
    "# | code-line-numbers: 1-7|8-14|15-18\n",
    "\n",
    "system_message = \"\"\"\n",
    "Given an input question, create a syntactically correct {dialect} query to\n",
    "run to help find the answer. Unless the user specifies in his question a\n",
    "specific number of examples they wish to obtain, always limit your query to\n",
    "at most {top_k} results. You can order the results by a relevant column to\n",
    "return the most interesting examples in the database.\n",
    "\n",
    "Never query for all the columns from a specific table, only ask for a the\n",
    "few relevant columns given the question.\n",
    "\n",
    "Pay attention to use only the column names that you can see in the schema\n",
    "description. Be careful to not query for columns that do not exist. Also,\n",
    "pay attention to which column is in which table.\n",
    "\n",
    "Only use the following tables:\n",
    "{table_info}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.small_text}\n",
    "\n",
    "[source {{< bi box-arrow-up-right >}}](https://python.langchain.com/docs/tutorials/sql_qa/#convert-question-to-sql-query)\n",
    "\n",
    ":::\n",
    "\n",
    "--- \n",
    "\n",
    "## The messages that constitute the prompt {.smaller}\n",
    "\n",
    "Now we construct a list of messages. These are `dicts` which have a key\n",
    "`user` or `system`, and a `content`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "output-location": "slide"
   },
   "outputs": [],
   "source": [
    "# | echo: true\n",
    "# | code-line-numbers: 4-10|11-14|19-23\n",
    "def generate_messages(question, dialect=\"SQL\", top_k=5, table_info=\"\"):\n",
    "    # Create a ChatPromptTemplate\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_message.format(\n",
    "                dialect=dialect, top_k=top_k, table_info=table_info\n",
    "            ),\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ]\n",
    "\n",
    "    return messages\n",
    "\n",
    "\n",
    "messages = generate_messages(\n",
    "    question=question, dialect=db.dialect, top_k=10, table_info=db.get_table_info()\n",
    ")\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "import json\n",
    "\n",
    "with open(\"messages.json\", \"w\") as fp:\n",
    "    json.dump(messages, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "### üîé the `system` message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "output-location": "default"
   },
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# | classes: scrollable\n",
    "print(messages[0][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: footer\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "## Calling the LLM with the prompt {.smaller}\n",
    "\n",
    "`litellm` is a Open Source Python library (and also a Proxy) that can be\n",
    "used to run inference and compute embeddings with a vast number\n",
    "of providers (new models added weekly).\n",
    "\n",
    "Among the provider lists we have:\n",
    "\n",
    "::: {.incremental}\n",
    "\n",
    "- Ollama / LMStudio / llama.cpp\n",
    "- IBM WatsonX.ai\n",
    "- Anthropic\n",
    "- OpenAI\n",
    "- AWS Sagemaker\n",
    "- OpenRouter\n",
    "- [and more {{< bi info-circle >}}](https://docs.litellm.ai/docs/providers)\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "### Using litellm\n",
    "\n",
    "::: {.fragment .fade-in-then-semi-out}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: true\n",
    "!uv add litellm --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::\n",
    "\n",
    "<!-- [^extras]:   \n",
    "  `caching` , `extra-proxy` , `mlflow` , `proxy` , `semantic-router`, `utils` -->\n",
    "\n",
    "\n",
    "::: {.fragment .fade-in}\n",
    "To run inference, we just call the [`completions`](https://docs.litellm.ai/docs/completion/input) module function:\n",
    "\n",
    "<!-- Enable disk cachesq -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# Speed up execution\n",
    "import litellm\n",
    "\n",
    "from litellm.caching.caching import Cache\n",
    "\n",
    "litellm.cache = Cache(type=\"disk\")\n",
    "# Remove debug messages\n",
    "litellm.suppress_debug_info = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: true\n",
    "\n",
    "import litellm\n",
    "\n",
    "# provider/model (if not local, ensure you have API üîë )\n",
    "model = \"watsonx/ibm/granite-3-2-8b-instruct\"\n",
    "\n",
    "response = litellm.completion(\n",
    "    model=model,\n",
    "    messages=messages,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::\n",
    "\n",
    "\n",
    "::: notes\n",
    "\n",
    "There are some reasonably good LLMs under the coder and instruct in Hugging Face\n",
    "Some of these can be run locally with some inference server like `Ollama`, \n",
    "`llama.cpp` or `LMStudio`, and also use pubic \n",
    "ones\n",
    "\n",
    ":::\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: true\n",
    "# | classes: scrollable\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Structured output {.smaller}\n",
    "\n",
    "::: {.fragment}\n",
    "LLM outputs are text, SQL for our use case.\n",
    "\n",
    "It's a good UX to give some explanation to the user about the LLM output. \n",
    "We will use `JSON Schema` output to have both output without any string manipulation ‚ú®\n",
    ":::\n",
    "\n",
    "::: {.fragment}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: true\n",
    "# | code-line-numbers: \"1|2-5|9-13|14-17\"\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class SQLOutput(BaseModel):\n",
    "    sql: str = Field(description=\"The SQL query\")\n",
    "    explanation: str = Field(description=\"The reasoning for the query construction\")\n",
    "\n",
    "\n",
    "# Optional\n",
    "# litellm.enable_json_schema_validation = True\n",
    "\n",
    "response = litellm.completion(\n",
    "    model=model,\n",
    "    messages=messages,\n",
    "    response_format=SQLOutput,\n",
    ")\n",
    "output = SQLOutput.model_validate_json(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::\n",
    "---\n",
    "\n",
    "### Attributes of `output` object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: true\n",
    "output.sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.fragment}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: true\n",
    "output.explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::\n",
    "\n",
    "\n",
    "\n",
    "--- \n",
    "\n",
    "### LLM generated SQL vs dataset query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: true\n",
    "display(query)\n",
    "display(output.sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execution result: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with engine.connect() as c:\n",
    "        response = c.execute(text(output.sql))\n",
    "        print(response.fetchall())\n",
    "except Exception as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Improving the Generation with more context\n",
    "\n",
    "The BIRD mini has some CSV files that help understanding\n",
    "the tables values and relationships\n",
    "\n",
    "Let's pass this to the prompt generation system message...\n",
    "\n",
    "#### TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# from cheap_repr import cheap_repr\n",
    "# messages.insert(1, {\n",
    "#   \"role\": \"\"\n",
    "# })\n",
    "# cheap_repr(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Create a workflow\n",
    "\n",
    "::: notes\n",
    "\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "# Chaining generation and execution\n",
    "\n",
    "::: {.fragment .fade-in-then-semi-out}\n",
    "<img src=\"./img/langgraph.svg\" height=\"40px\" style=\"float: left; padding-top: .5em; padding-right: .25em\"/> \n",
    "<p>`LangGraph` allows us to create state machines around LLM calls, using\n",
    "functions as nodes.</p>\n",
    ":::\n",
    "\n",
    "::: {.fragment .fade-in-then-semi-out}\n",
    "We will now put the code we saw into node functions\n",
    ":::\n",
    "\n",
    "::: {.fragment .fade-in-then-semi-out}\n",
    "We will have a `Input`, and `Output` a shared `State` and some global configuration, \n",
    "instead of global variables\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "![](./img/LLMsSQL.svg)\n",
    "\n",
    "TODO highlight\n",
    "\n",
    "---\n",
    "\n",
    "## Graph creation\n",
    "\n",
    "```{.python include=\"src/sql_with_llms/main.py\" snippet=\"graph\" dedent=\"4\" code-line-numbers=\"false\"}\n",
    "```\n",
    "\n",
    "::: {.fragment}\n",
    "Let's take a look at how we define those function and the data types\n",
    "they receive and return\n",
    ":::\n",
    "\n",
    "\n",
    "::: notes\n",
    "1. In Langraph the node functions must be defined before, but we're presenting this\n",
    "   before for clarity\n",
    ":::\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# | output: true\n",
    "\n",
    "from sql_with_llms.main import create_graph\n",
    "from IPython.display import Markdown\n",
    "\n",
    "graph = create_graph()\n",
    "# display(Markdown(\"```{mermaid}\\n%s\\n```\" % graph.get_graph().draw_mermaid()))\n",
    "\n",
    "Markdown(\n",
    "    dedent(\"\"\"\n",
    "  ### A\n",
    "\n",
    "  ### B\n",
    "\n",
    "  \"\"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Input class\n",
    "\n",
    "This is what we expect to be passed to our state machine \n",
    "\n",
    "```{.python include=\"src/sql_with_llms/main.py\" snippet=\"input\"}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## State class\n",
    "\n",
    "This is what the state machine will pass to the nodes (functions)\n",
    "and expect from them (except the first and last node which use `Input`/`Output`)\n",
    "\n",
    "```{.python include=\"src/sql_with_llms/main.py\" snippet=\"state\" code-line-numbers=\"false\"}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Output class\n",
    "\n",
    "This is the response back to the user, we will use it in the *last* node \n",
    "```{.python include=\"src/sql_with_llms/main.py\" snippet=\"output\" code-line-numbers=\"false\"}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## The init node function\n",
    "\n",
    "This node consumes the input converting the something of shape `Input` into\n",
    "a `State`. The next node will be prompt generation, so we will be save the\n",
    "`question`.\n",
    "\n",
    "```{.python include=\"src/sql_with_llms/main.py\" snippet=\"init\" code-line-numbers=\"false\"}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Global configuration class\n",
    "\n",
    "The nodes beyond the initial one will require to have a bit more information\n",
    "that won't change (not considered state), for this we will define a global \n",
    "```{.python include=\"src/sql_with_llms/main.py\" snippet=\"cfg\" code-line-numbers=\"false\"}\n",
    "```\n",
    "We will pass it to the function nodes with `functools.partial(..., conf=conf)` \n",
    "\n",
    "---\n",
    "\n",
    "## Prompt generation function node\n",
    "\n",
    "```{.python include=\"src/sql_with_llms/main.py\" snippet=\"prompt_gen\" code-line-numbers=\"false\"}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "![](./img/LLMsSQL.svg)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## LLM calling function node\n",
    "\n",
    "\n",
    "```{.python include=\"src/sql_with_llms/main.py\" snippet=\"call_llm\" code-line-numbers=\"false\"}\n",
    "```\n",
    "\n",
    "\n",
    "## SQL Execution function node\n",
    "\n",
    "```{.python include=\"src/sql_with_llms/main.py\" snippet=\"exec_sql\" code-line-numbers=\"false\"}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# Lessons learned\n",
    "\n",
    "---\n",
    "\n",
    "### Lessons learned \n",
    "\n",
    "::: {.fragment}\n",
    "- Tables with a high number of columns can make the context larger\n",
    "  - RAG at the column level is a common technique to improve efficiency and accuracy.\n",
    ":::\n",
    "::: {.fragment}\n",
    "- SQL manipulation at the abstract syntax tree level (`sqlglot`)\n",
    "  - Finding dangerous operations (DML)\n",
    "  - Value substitution \n",
    ":::\n",
    "\n",
    "\n",
    "::: notes\n",
    "Also we can pass the SQL to SQLAlchemy engine to validate if the syntax is correct\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "### Lessons learned  (cont.)\n",
    "\n",
    "::: {.fragment}\n",
    "- Please more context!\n",
    "  - Adding examples, but not too many, or only the relevant ones ü§î\n",
    ":::\n",
    "\n",
    "::: {.fragment}\n",
    "- Dynamic context\n",
    "\n",
    "  - date based\n",
    "  - external resources\n",
    "  - AI tools\n",
    ":::\n",
    "\n",
    "\n",
    "::: footer\n",
    "For example, the term Q1 can mean, *quarter one*, but it should\n",
    "    be supplied dynamically\n",
    ":::\n",
    "---\n",
    "\n",
    "### Lessons learned (cont.)\n",
    "\n",
    "::: {.fragment}\n",
    "- Function nodes\n",
    "  - Custom modifications \n",
    ":::\n",
    "\n",
    "\n",
    "::: notes\n",
    "\n",
    "## Extensibilit\n",
    "- Offload node responsibilities into external services (e.g. DB execution) \n",
    "- Pipelines defined as YAML\n",
    "- Use Python's built in `entry-point` system to *install* nodes\n",
    "  This is a good approach to separate production from \n",
    ":::\n",
    "---\n",
    "\n",
    "\n",
    "## Thank you & Questions{.center}\n",
    "\n",
    "::: {.columns }\n",
    "::: {.column width=\"36%\"}\n",
    ":::\n",
    "::: {.column width=\"30%\"}\n",
    "{{< qrcode https://d3f0.github.io/pycon-sql-llm/slides.html#/title-slide qr2 width=400 height=400 colorDark='#0011bb' >}}\n",
    ":::\n",
    "::: {.column width=\"30%\"}\n",
    ":::\n",
    ":::\n",
    "\n",
    "::: {style=\"text-align: center; \"}\n",
    "{{< bi envelope >}} [nahuel.defosse@ibm.com](mailto:nahuel.defosse@ibm.com)\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3",
   "path": "/Users/nahueldefosse/workspace/slides/pycon-2025-insights-from-sql-llms/.venv/share/jupyter/kernels/python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
